# Dolphin29 - Uncensored LLM Docker Stack

A Docker Compose setup for running Ollama with the Dolphin 2.9 uncensored model and structured output capabilities.

## Features

- üê≥ One-command Docker deployment
- üîì Uncensored AI responses
- üìä Structured JSON output support
- ‚ö° GPU acceleration support
- üõ†Ô∏è Automatic model setup

## Quick Start

1. Clone the repository:
```bash
git clone https://github.com/sammrai/dolphin29.git
cd dolphin29
```

2. Start the service:
```bash
docker compose up --build -d
```

3. Wait for initial model download (first run only), then test:
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "dolphin29-uncensored",
    "prompt": "Hello, how are you?",
    "stream": false
  }'
```

## Structured Output Example

```bash
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "dolphin29-uncensored",
    "messages": [{"role": "user", "content": "Analyze this data and provide structured output"}],
    "format": {
      "type": "object",
      "properties": {
        "analysis": {"type": "string"},
        "confidence": {"type": "number"}
      }
    }
  }'
```

## Requirements

- Docker & Docker Compose
- NVIDIA GPU (optional, for acceleration)
- 4-6GB available VRAM/RAM

## Model Information

- **Base Model**: Dolphin Llama3 8B
- **Custom Model**: dolphin29-uncensored
- **Features**: Uncensored responses, structured output support
- **Size**: ~4.7GB

## Configuration

The setup uses a custom Modelfile with an uncensored system prompt. Models are automatically downloaded on first run.

## License

MIT License - see LICENSE file for details.

## Contributing

Pull requests welcome. Please ensure all changes are tested with the Docker environment.

## Disclaimer

This tool is for research and development purposes. Users are responsible for compliance with applicable laws and ethical guidelines.